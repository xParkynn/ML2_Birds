{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe26fe16",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn               \n",
    "import torch.nn.functional as F       \n",
    "import torch.optim as optim            \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "#\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T   \n",
    "import numpy as np                     \n",
    "import pandas as pd                   \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09963657",
   "metadata": {},
   "source": [
    "# BASE MODEL ARCHITECTURE - test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98965b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # Dynamically determine the correct input size for the Linear layer\n",
    "        self.flattened_size = self._get_flattened_size()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def _get_flattened_size(self):\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, 128, 801) # Same shape as input chunks\n",
    "            out = self.conv(dummy)\n",
    "            return out.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf8029",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85330c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "INIT_LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "BATCH_SIZE = 512\n",
    "LOSS = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "\n",
    "#model = CNNClassifier(num_classes=206)\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=INIT_LR, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d50988",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c659348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class SpectogramDataset(Dataset):\n",
    "    def __init__(self, audio_dir:str, label_to_idx:dict, max_cache_size: int = 5):\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.audio_dir = audio_dir\n",
    "        self.chunk_index_pairs = []\n",
    "        self.cache = OrderedDict()\n",
    "        self.max_cache_size = max_cache_size\n",
    "\n",
    "        with open('./dataset_init.json', 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        for path in self.audio_dir:\n",
    "            label = os.path.basename(path).replace(\".pt\", \"\")\n",
    "            amount_of_chunks = data[label]\n",
    "            for n in range(amount_of_chunks):\n",
    "                self.chunk_index_pairs.append((path, label.split('_')[0], n))\n",
    "\n",
    "    def load_cached_tensor(self, file_path):\n",
    "        if file_path in self.cache:\n",
    "            self.cache.move_to_end(file_path)\n",
    "        else:\n",
    "            tensor = torch.load(file_path)\n",
    "            self.cache[file_path] = tensor\n",
    "            if len(self.cache) > self.max_cache_size:\n",
    "                self.cache.popitem(last=False)\n",
    "        return self.cache[file_path]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunk_index_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        file_path, label, chunk_index = self.chunk_index_pairs[idx]\n",
    "        tensor = self.load_cached_tensor(file_path)\n",
    "        chunk = tensor[chunk_index].to(torch.float32)\n",
    "        \n",
    "        label_index = self.label_to_idx[label]\n",
    "        \n",
    "        #if chunk.size(0) == 1:\n",
    "        #    chunk = chunk.repeat(3,1,1)\n",
    "        #chunk = F.interpolate(chunk.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        return chunk, label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c400de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_csv(\"./data/processed_data.csv\")\n",
    "unique_labels = sorted(metadata[\"primary_label\"].astype(str).unique())\n",
    "label_to_index = {label: idx for idx,label in enumerate(unique_labels)}\n",
    "index_to_label = {idx: label for idx,label in enumerate(unique_labels)}\n",
    "print(list(unique_labels)[:10])\n",
    "#kann auch numerisch sorten theoretisch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b58905",
   "metadata": {},
   "source": [
    "# OLD-TEST Überarbeitetes  ChunkDataset von ino (path issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa62c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class inoChunkedSpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_list:list, label_to_idx: dict):\n",
    "        self.file_list = file_list\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.index_list = []\n",
    "\n",
    "       \n",
    "        for tensor_path in self.file_list:\n",
    "            label = os.path.basename(tensor_path).replace(\".pt\", \"\")\n",
    "            try:\n",
    "                tensor = torch.load(tensor_path, map_location=\"cpu\")\n",
    "                n_chunks = tensor.shape[0]\n",
    "                for i in range(n_chunks):\n",
    "                    self.index_list.append((tensor_path, label, i))\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Laden von {tensor_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path, label_name, chunk_idx = self.index_list[idx]\n",
    "        try:\n",
    "            all_chunks = torch.load(tensor_path, map_location=\"cpu\")\n",
    "            chunk = all_chunks[chunk_idx]\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden von Chunk {chunk_idx} für Label {label_name}: {e}\")\n",
    "            chunk = torch.zeros((1, 128, 216))\n",
    "        if label_name in self.label_to_idx:\n",
    "            label_index = self.label_to_idx[label_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Unbekanntes Label: {label_name}\")\n",
    "\n",
    "        return chunk, label_index "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478682e6",
   "metadata": {},
   "source": [
    "# DataLoader for ChunkSpectogramDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_csv(\"./data/processed_data.csv\")\n",
    "unique_labels = sorted(metadata[\"primary_label\"].astype(str).unique())\n",
    "label_to_idx = {label: idx for idx,label in enumerate(unique_labels)}\n",
    "print(len(label_to_idx.keys()))\n",
    "\n",
    "tensor_dir = \"./data/processed_train_audio/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the tensor from file\n",
    "tensor = torch.load('./data/processed_train_audio/21038/iNat65519.pt')\n",
    "# Print the shape\n",
    "print(tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe095532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "import os \n",
    "import random \n",
    "\n",
    "train_files, test_files = [], []\n",
    "label_to_idx = dict()\n",
    "for idx, file in enumerate(sorted(os.listdir('./data/processed_train_audio'))):\n",
    "    path = f'./data/processed_train_audio/{file}'\n",
    "\n",
    "    if '_train' in file:\n",
    "        train_files.append(path)\n",
    "        label_to_idx[file.split('_')[0]] = idx//2\n",
    "\n",
    "    elif '_test' in file:\n",
    "        test_files.append(path)\n",
    "\n",
    "\n",
    "\n",
    "assert not set(train_files).intersection(set(test_files))\n",
    "assert len(label_to_idx.keys()) == 205\n",
    "#train_dataset = inoChunkedSpectrogramDataset(train_files, label_to_idx)\n",
    "#test_dataset = inoChunkedSpectrogramDataset(test_files, label_to_idx)\n",
    "\n",
    "train_dataset = SpectogramDataset(train_files, label_to_idx)\n",
    "test_dataset = SpectogramDataset(test_files, label_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5e623",
   "metadata": {},
   "source": [
    "# DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers = 3, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2629b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers = 3, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931e2b4",
   "metadata": {},
   "source": [
    "# Train Loop inoChunkedSpectogramDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import tqdm\n",
    "\n",
    "loss_fn = LOSS.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in tqdm.tqdm(enumerate(train_loader)):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  # shape: [batch, num_classes]\n",
    "        loss = loss_fn(output, target)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm.tqdm(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    print(f\"\\nEpoch: {epoch}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('./data/processed_train_audio'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4feac0",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e841a8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 71 (26651095.py, line 72)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mf1 = f1_score(all_labels, all_preds, average='samples')\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'if' statement on line 71\n"
     ]
    }
   ],
   "source": [
    "resnet_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "for name, layer in resnet_model.named_parameters():\n",
    "    if not ('fc' in name or 'layer4' in name or 'layer3' in name):\n",
    "        layer.requires_grad = False\n",
    "\n",
    "resnet_model.fc = nn.Sequential(\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 205)\n",
    ")\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "RESNET_EPOCHS = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=256, shuffle=True, pin_memory=True, num_workers = 2, persistent_workers=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, pin_memory=True, num_workers = 2, persistent_workers=False)\n",
    "resnet_model = resnet_model.to(device)\n",
    "optimizer = torch.optim.AdamW(resnet_model.parameters(), weight_decay=WEIGHT_DECAY, lr=3e-3)\n",
    "\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "for epoch in range(RESNET_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    test_loss = 0\n",
    "    resnet_model.train()\n",
    "    for idx, (data, label) in tqdm.tqdm(enumerate(train_loader)):\n",
    "        data, label = data.to(device, non_blocking=True), label.float().to(device, non_blocking=True)\n",
    "        for label_tens in label:\n",
    "            assert label_tens.sum(dim=-1) >=1\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device):\n",
    "            model_output = resnet_model(data)\n",
    "            loss_value = loss_fn(model_output, label)\n",
    "        scaler.scale(loss_value).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss_value.item()\n",
    "\n",
    "    print(f\"Loss of {epoch} epoch: {epoch_loss/len(train_loader)}\")\n",
    "\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "    resnet_model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, label) in tqdm.tqdm(enumerate(test_loader)):\n",
    "            data, label = data.to(device, non_blocking=True), label.float().to(device, non_blocking=True)\n",
    "            test_output = resnet_model(data)\n",
    "            loss_value = loss_fn(test_output, label)\n",
    "            test_loss += loss_value.item()\n",
    "            prediction = (torch.sigmoid(test_output) > 0.5).float()\n",
    "            if epoch == 0 and idx == 0:\n",
    "                print(\"Raw logits:\", test_output[0][:10].cpu())\n",
    "                print(\"Sigmoid:\", torch.sigmoid(test_output[0])[:10].cpu())\n",
    "                print(\"Prediction:\", prediction[0][:10])\n",
    "                print(\"Label:     \", label[0][:10])\n",
    "                print(\"Prediction unique:\", prediction.unique())\n",
    "                print(\"Label unique:\", label.unique())\n",
    "            all_preds.append(prediction.cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "        \n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_labels = torch.cat(all_labels).numpy()\n",
    "        if epoch == 0:\n",
    "            for thresh in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "                preds = (torch.sigmoid(torch.tensor(all_preds)) > thresh).numpy()\n",
    "                f1 = f1_score(all_labels, preds, average='samples')\n",
    "                print(f\"Threshold {thresh:.1f} → F1: {f1:.4f}\")\n",
    "        f1 = f1_score(all_labels, all_preds, average='samples')\n",
    "        print(f'Accuracy of {epoch}th epoch: {f1:.4f}')\n",
    "        print(f'Test-Loss of {epoch}th epoch: {test_loss/len(test_loader)}')\n",
    "\n",
    "        \n",
    "torch.save(resnet_model.state_dict(), './resnet_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35904da2",
   "metadata": {},
   "source": [
    "# Map von id to name für die Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e939abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "taxonomy_df = pd.read_csv(\"./data/taxonomy.csv\")\n",
    "\n",
    "id_to_name = {}\n",
    "for index, row in taxonomy_df.iterrows():\n",
    "    primary_label = row[\"primary_label\"]\n",
    "    common_name = row[\"common_name\"] \n",
    "    id_to_name[primary_label] = common_name\n",
    "\n",
    "#für die visualisierung später \n",
    "\n",
    "print(list(id_to_name.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531db48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
