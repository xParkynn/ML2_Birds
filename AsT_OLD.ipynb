{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19354f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn               \n",
    "import torch.nn.functional as F       \n",
    "import torch.optim as optim            \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import timm \n",
    "#\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T   \n",
    "import numpy as np                     \n",
    "import pandas as pd                   \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a525e74",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dad8fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "INIT_LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "BATCH_SIZE = 64\n",
    "LOSS = torch.nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fcc960",
   "metadata": {},
   "source": [
    "# Dataset init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9aef81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# import torch.nn.functional as F\n",
    "# import os\n",
    "# import json\n",
    "# from collections import OrderedDict\n",
    "# import torchvision.transforms as T\n",
    "\n",
    "# class AsTSpectogramDataset(Dataset):\n",
    "#     def __init__(self, audio_dir, label_to_idx, max_cache_size=5):\n",
    "#         self.label_to_idx = label_to_idx\n",
    "#         self.audio_dir = audio_dir\n",
    "#         self.chunk_index_pairs = []\n",
    "#         self.cache = OrderedDict()\n",
    "#         self.max_cache_size = max_cache_size\n",
    "\n",
    "#         with open('./dataset_init.json', 'r') as file:\n",
    "#             data = json.load(file)\n",
    "\n",
    "#         for path in self.audio_dir:\n",
    "#             label = os.path.basename(path).replace(\".pt\", \"\")\n",
    "#             amount_of_chunks = data[label]\n",
    "#             for n in range(amount_of_chunks):\n",
    "#                 self.chunk_index_pairs.append((path, label.split('_')[0], n))\n",
    "\n",
    "\n",
    "#     def load_cached_tensor(self, file_path):\n",
    "#         if file_path in self.cache:\n",
    "#             self.cache.move_to_end(file_path)\n",
    "#         else:\n",
    "#             tensor = torch.load(file_path)\n",
    "#             self.cache[file_path] = tensor\n",
    "#             if len(self.cache) > self.max_cache_size:\n",
    "#                 self.cache.popitem(last=False)\n",
    "#         return self.cache[file_path]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.chunk_index_pairs)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         file_path, label, chunk_index = self.chunk_index_pairs[idx]\n",
    "#         tensor = self.load_cached_tensor(file_path)\n",
    "        \n",
    "#         chunk = tensor[chunk_index].to(torch.float32)\n",
    "#         if chunk.size(0) == 3:\n",
    "#             chunk = chunk[0].unsqueeze(0)  # Von [3, H, W] zu [1, H, W]\n",
    "\n",
    "#         chunk = F.interpolate(chunk.unsqueeze(0), size=(128, 1024), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "#         if label in self.label_to_idx:\n",
    "#             label_index = self.label_to_idx[label]\n",
    "#         else:\n",
    "#             raise ValueError(f\"Label '{label}' not found in label_to_idx\")\n",
    "#         print(\"Final shape:\", chunk.shape)  # Sollte [1, 128, 1024] sein\n",
    "#         return chunk, label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8eeb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "\n",
    "class ASTRawAudioDataset(Dataset):\n",
    "    def __init__(self, file_label_pairs, label_to_idx, feature_extractor, target_sr=16000):\n",
    "        self.file_label_pairs = file_label_pairs\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.target_sr = target_sr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_label_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.file_label_pairs[idx]\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "        if sr != self.target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        waveform = waveform.mean(dim=0)  \n",
    "\n",
    "\n",
    "        inputs = self.feature_extractor(\n",
    "            [waveform.numpy()],\n",
    "            sampling_rate=self.target_sr,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        input_values = inputs[\"input_values\"].squeeze(0)  \n",
    "        label_index = self.label_to_idx[label]\n",
    "\n",
    "        return input_values, label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e0fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([206]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([206, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#versucht aber hat ned geklappt \n",
    "import torch\n",
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "extractor_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    extractor_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "feature_extractor.return_attention_mask = True\n",
    "feature_extractor.max_length = 1024  \n",
    "feature_extractor.padding_value = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    waveforms, labels = zip(*batch)  \n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        list(waveforms),               \n",
    "        sampling_rate=16000,           \n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "    input_values = inputs[\"input_values\"].to(device)\n",
    "    labels = torch.tensor(labels).to(device)\n",
    "    return input_values, labels\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    extractor_name,\n",
    "    num_labels=len(label_to_idx),\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ade411",
   "metadata": {},
   "source": [
    "# Train, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "base_dir = './data/train_audio'\n",
    "label_to_idx = {}\n",
    "train_files = []\n",
    "test_files = []\n",
    "test_split_ratio = 0.2  \n",
    "\n",
    "all_labels = sorted(os.listdir(base_dir))\n",
    "\n",
    "for idx, label in enumerate(all_labels):\n",
    "    label_dir = os.path.join(base_dir, label)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue  \n",
    "\n",
    "    label_to_idx[label] = idx\n",
    "    audio_files = [os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.ogg')]\n",
    "\n",
    "    #split n shuffle\n",
    "    random.shuffle(audio_files)\n",
    "    split_point = int(len(audio_files) * (1 - test_split_ratio))\n",
    "    train_files.extend([(f, label) for f in audio_files[:split_point]])\n",
    "    test_files.extend([(f, label) for f in audio_files[split_point:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c604c",
   "metadata": {},
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "54e40205",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ASTRawAudioDataset(train_files, label_to_idx, feature_extractor)\n",
    "test_dataset = ASTRawAudioDataset(test_files, label_to_idx, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a41d620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = BATCH_SIZE\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "88b0979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 22777\n",
      "Test dataset size: 5787\n",
      "Sample shape: torch.Size([1024, 128])\n",
      "Label index: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))\n",
    "\n",
    "sample, label_index = train_dataset[0]\n",
    "print(\"Sample shape:\", sample.shape)\n",
    "print(\"Label index:\", label_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "53d2b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 0/356 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "choose a window size 400 that is [2, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m target = target.to(device)               \u001b[38;5;66;03m# shape: [B]\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# STEP 1: Run feature extractor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m inputs = \u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# STEP 2: Forward pass\u001b[39;00m\n\u001b[32m     25\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py:219\u001b[39m, in \u001b[36mASTFeatureExtractor.__call__\u001b[39m\u001b[34m(self, raw_speech, sampling_rate, return_tensors, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m     raw_speech = [raw_speech]\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# extract fbank features and pad/truncate to max_length\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m features = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_fbank_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_speech\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# convert into BatchFeature\u001b[39;00m\n\u001b[32m    222\u001b[39m padded_inputs = BatchFeature({\u001b[33m\"\u001b[39m\u001b[33minput_values\u001b[39m\u001b[33m\"\u001b[39m: features})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py:219\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    216\u001b[39m     raw_speech = [raw_speech]\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# extract fbank features and pad/truncate to max_length\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m features = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_fbank_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m waveform \u001b[38;5;129;01min\u001b[39;00m raw_speech]\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# convert into BatchFeature\u001b[39;00m\n\u001b[32m    222\u001b[39m padded_inputs = BatchFeature({\u001b[33m\"\u001b[39m\u001b[33minput_values\u001b[39m\u001b[33m\"\u001b[39m: features})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py:119\u001b[39m, in \u001b[36mASTFeatureExtractor._extract_fbank_features\u001b[39m\u001b[34m(self, waveform, max_length)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_speech_available():\n\u001b[32m    118\u001b[39m     waveform = torch.from_numpy(waveform).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     fbank = \u001b[43mta_kaldi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfbank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_frequency\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwindow_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhanning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_mel_bins\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_mel_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    126\u001b[39m     waveform = np.squeeze(waveform)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torchaudio/compliance/kaldi.py:591\u001b[39m, in \u001b[36mfbank\u001b[39m\u001b[34m(waveform, blackman_coeff, channel, dither, energy_floor, frame_length, frame_shift, high_freq, htk_compat, low_freq, min_duration, num_mel_bins, preemphasis_coefficient, raw_energy, remove_dc_offset, round_to_power_of_two, sample_frequency, snip_edges, subtract_mean, use_energy, use_log_fbank, use_power, vtln_high, vtln_low, vtln_warp, window_type)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Create a fbank from a raw audio signal. This matches the input/output of Kaldi's\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03mcompute-fbank-feats.\u001b[39;00m\n\u001b[32m    544\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    587\u001b[39m \u001b[33;03m    where m is calculated in _get_strided\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    589\u001b[39m device, dtype = waveform.device, waveform.dtype\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m waveform, window_shift, window_size, padded_window_size = \u001b[43m_get_waveform_and_window_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_shift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_to_power_of_two\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreemphasis_coefficient\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(waveform) < min_duration * sample_frequency:\n\u001b[32m    596\u001b[39m     \u001b[38;5;66;03m# signal is too short\u001b[39;00m\n\u001b[32m    597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.empty(\u001b[32m0\u001b[39m, device=device, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torchaudio/compliance/kaldi.py:142\u001b[39m, in \u001b[36m_get_waveform_and_window_properties\u001b[39m\u001b[34m(waveform, channel, sample_frequency, frame_shift, frame_length, round_to_power_of_two, preemphasis_coefficient)\u001b[39m\n\u001b[32m    139\u001b[39m window_size = \u001b[38;5;28mint\u001b[39m(sample_frequency * frame_length * MILLISECONDS_TO_SECONDS)\n\u001b[32m    140\u001b[39m padded_window_size = _next_power_of_2(window_size) \u001b[38;5;28;01mif\u001b[39;00m round_to_power_of_two \u001b[38;5;28;01melse\u001b[39;00m window_size\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[32m2\u001b[39m <= window_size <= \u001b[38;5;28mlen\u001b[39m(waveform), \u001b[33m\"\u001b[39m\u001b[33mchoose a window size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m that is [2, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    143\u001b[39m     window_size, \u001b[38;5;28mlen\u001b[39m(waveform)\n\u001b[32m    144\u001b[39m )\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[32m0\u001b[39m < window_shift, \u001b[33m\"\u001b[39m\u001b[33m`window_shift` must be greater than 0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m padded_window_size % \u001b[32m2\u001b[39m == \u001b[32m0\u001b[39m, (\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mthe padded `window_size` must be divisible by two.\u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33m use `round_to_power_of_two` or change `frame_length`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    148\u001b[39m )\n",
      "\u001b[31mAssertionError\u001b[39m: choose a window size 400 that is [2, 64]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_model_path = \"best_ast_model.pth\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (waveform, target) in enumerate(tqdm(train_loader, desc=f\"Training epoch {epoch}\")):\n",
    "        waveform = waveform.to(device)           \n",
    "        target = target.to(device)               \n",
    "\n",
    "       \n",
    "        inputs = feature_extractor(\n",
    "            waveform,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "\n",
    "   \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "\n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch} Train loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for waveform, target in tqdm(test_loader, desc=f\"Evaluating epoch {epoch}\"):\n",
    "            waveform = waveform.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            inputs = feature_extractor(\n",
    "                waveform,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    prec = precision_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved at epoch {epoch} with F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40728c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
