{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a59a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from itertools import repeat\n",
    "import os\n",
    "\n",
    "\n",
    "class AstDataset(Dataset):\n",
    "    def __init__(self, mode: str, dir: str):\n",
    "        self.tensors = []\n",
    "        self.labels = []\n",
    "        for idx, file in enumerate(sorted(os.listdir(dir))):\n",
    "            if mode in file:\n",
    "                label = file.split('_')[0]\n",
    "                tensor = torch.load(os.path.join(dir, file))\n",
    "                self.tensors.append(tensor)\n",
    "                self.labels.extend([idx//2] * tensor.shape[0])\n",
    "        \n",
    "        self.tensors = torch.cat(self.tensors)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensors[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "train_dataset = AstDataset('train', './data/train_chunks')\n",
    "test_dataset = AstDataset('test', './data/train_chunks')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=6, persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=6, pin_memory=True, persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a50aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/miniforge3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Train-Batches: 100%|██████████| 788/788 [05:04<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 1th Epoch: 4.7190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 533/533 [01:16<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 1th Epoch: 5.1451\n",
      "Accuracy of Epoch 1: 0.007653510058061111\n",
      "F1-Score of Epoch 1: 0.006013439587123013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 788/788 [05:01<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 2th Epoch: 3.9556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 533/533 [01:13<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 2th Epoch: 5.1938\n",
      "Accuracy of Epoch 2: 0.010409946630696146\n",
      "F1-Score of Epoch 2: 0.007146739040374429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches:  18%|█▊        | 138/788 [00:53<04:12,  2.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m     epoch_train_loss += loss_val.item()\n\u001b[32m     44\u001b[39m     scaler.scale(loss_val).backward()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     scaler.update()\n\u001b[32m     48\u001b[39m avg_epoch_train_loss = epoch_train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/amp/grad_scaler.py:457\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    454\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    455\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/amp/grad_scaler.py:351\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/amp/grad_scaler.py:351\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "labels = train_dataset.labels.numpy()\n",
    "\n",
    "\n",
    "model = transformers.ViTForImageClassification.from_pretrained(model_name, num_labels=len(np.unique(labels)))\n",
    "\n",
    "model.classifier = nn.Linear(model.classifier.in_features, len(np.unique(labels)))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "class_counts = np.bincount(labels)\n",
    "class_weights = 1. / (class_counts+1e-9)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "best_f1 = 0\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "scaler = GradScaler(device)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    epoch_train_loss, epoch_test_loss = 0,0\n",
    "\n",
    "    for data, label in tqdm(train_loader, desc='Train-Batches'):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device):\n",
    "            train_output = model(data).logits\n",
    "            loss_val = loss_fn(train_output, label)\n",
    "        \n",
    "        epoch_train_loss += loss_val.item()\n",
    "        scaler.scale(loss_val).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    avg_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    print(f\"Average Epoch Train Loss of {epoch+1}th Epoch: {avg_epoch_train_loss:.4f}\")\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, label in tqdm(test_loader, desc='Test-Batches'):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            with autocast(device):\n",
    "                test_output = model(data).logits\n",
    "                test_loss = loss_fn(test_output, label)\n",
    "            epoch_test_loss += test_loss.item()\n",
    "            prediction = test_output.argmax(dim=1)\n",
    "            all_preds.append(prediction.cpu().numpy())\n",
    "            all_labels.append(label.cpu().numpy())\n",
    "\n",
    "    all_preds, all_labels = np.concatenate(all_preds), np.concatenate(all_labels)\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    avg_epoch_test_loss = epoch_test_loss/len(test_loader)\n",
    "    print(f'Average Epoch Test Loss of {epoch+1}th Epoch: {avg_epoch_test_loss:.4f}')\n",
    "    print(f'Accuracy of Epoch {epoch+1}: {acc}')\n",
    "    print(f'F1-Score of Epoch {epoch+1}: {f1}')\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        torch.save(model.state_dict(), './bestViT.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fe05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 98 unique labels for train mode.\n",
      "Loaded 29400 total train chunks.\n",
      "Discovered 98 unique labels for test mode.\n",
      "Loaded 22280 total test chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Train-Batches: 100%|██████████| 460/460 [23:11<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 1th Epoch: 4.4282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [12:43<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 1th Epoch: 4.2525\n",
      "Accuracy of Epoch 1: 0.19389587073608616\n",
      "F1-Score of Epoch 1: 0.1524869902111614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [23:22<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 2th Epoch: 4.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [12:30<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 2th Epoch: 3.9321\n",
      "Accuracy of Epoch 2: 0.3102333931777379\n",
      "F1-Score of Epoch 2: 0.26077011920176313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [23:25<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 3th Epoch: 3.7154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [12:29<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 3th Epoch: 3.6738\n",
      "Accuracy of Epoch 3: 0.37307001795332134\n",
      "F1-Score of Epoch 3: 0.3285696619201988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [23:11<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 4th Epoch: 3.4199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [12:29<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 4th Epoch: 3.4386\n",
      "Accuracy of Epoch 4: 0.4026032315978456\n",
      "F1-Score of Epoch 4: 0.35787172807018613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [23:22<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 5th Epoch: 3.1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [12:26<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 5th Epoch: 3.2167\n",
      "Accuracy of Epoch 5: 0.43994614003590665\n",
      "F1-Score of Epoch 5: 0.39969874424529733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [23:30<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 6th Epoch: 2.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [12:33<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 6th Epoch: 3.0262\n",
      "Accuracy of Epoch 6: 0.46247755834829446\n",
      "F1-Score of Epoch 6: 0.4284819388904043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [23:31<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 7th Epoch: 2.5941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [12:31<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 7th Epoch: 2.8467\n",
      "Accuracy of Epoch 7: 0.47630161579892283\n",
      "F1-Score of Epoch 7: 0.4432394514856658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [23:23<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 8th Epoch: 2.3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [12:36<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 8th Epoch: 2.7131\n",
      "Accuracy of Epoch 8: 0.4902603231597846\n",
      "F1-Score of Epoch 8: 0.4639827462455889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [23:51<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 9th Epoch: 2.0943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [13:09<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 9th Epoch: 2.5667\n",
      "Accuracy of Epoch 9: 0.5052513464991023\n",
      "F1-Score of Epoch 9: 0.47879413621833977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [24:06<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 10th Epoch: 1.8548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [13:01<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 10th Epoch: 2.4718\n",
      "Accuracy of Epoch 10: 0.5140035906642729\n",
      "F1-Score of Epoch 10: 0.4909868191890801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches: 100%|██████████| 460/460 [24:22<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Train Loss of 11th Epoch: 1.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test-Batches: 100%|██████████| 349/349 [12:55<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Epoch Test Loss of 11th Epoch: 2.3782\n",
      "Accuracy of Epoch 11: 0.5175942549371634\n",
      "F1-Score of Epoch 11: 0.495018972724812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train-Batches:   0%|          | 2/460 [00:07<28:01,  3.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    123\u001b[39m model.train()\n\u001b[32m    124\u001b[39m epoch_train_loss, epoch_test_loss = \u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTrain-Batches\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# data hat jetzt die Form (Batch_Size, 3, 224, 224)\u001b[39;49;00m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mAstDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     51\u001b[39m file_path, chunk_idx, label_id = \u001b[38;5;28mself\u001b[39m.data_info[idx]\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Load the full tensor for the class\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# This will be done per worker, and potentially cached by PyTorch's DataLoader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m full_class_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Select the specific chunk\u001b[39;00m\n\u001b[32m     58\u001b[39m data_chunk = full_class_tensor[chunk_idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/serialization.py:1462\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1462\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1470\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/serialization.py:1964\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   1962\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   1963\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1965\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1967\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/_weights_only_unpickler.py:512\u001b[39m, in \u001b[36mUnpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    505\u001b[39m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[32m    506\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) > \u001b[32m0\u001b[39m\n\u001b[32m    507\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.serialization._maybe_decode_ascii(pid[\u001b[32m0\u001b[39m]) != \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    508\u001b[39m     ):\n\u001b[32m    509\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[32m    510\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[32m0\u001b[39m], LONG_BINGET[\u001b[32m0\u001b[39m]]:\n\u001b[32m    514\u001b[39m     idx = (read(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[32m0\u001b[39m] == BINGET[\u001b[32m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, read(\u001b[32m4\u001b[39m)))[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/serialization.py:1928\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/nlp/lib/python3.12/site-packages/torch/serialization.py:1888\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   1885\u001b[39m     storage = overall_storage[storage_offset : storage_offset + numel]\n\u001b[32m   1886\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1887\u001b[39m     storage = (\n\u001b[32m-> \u001b[39m\u001b[32m1888\u001b[39m         \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1889\u001b[39m         ._typed_storage()\n\u001b[32m   1890\u001b[39m         ._untyped_storage\n\u001b[32m   1891\u001b[39m     )\n\u001b[32m   1892\u001b[39m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[32m   1893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np \n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "class AstDataset(Dataset):\n",
    "    def __init__(self, mode: str, data_dir: str):\n",
    "        self.data_info = [] \n",
    "        self.label_to_id = {}\n",
    "        current_id = 0\n",
    "        \n",
    "\n",
    "        unique_label_names = sorted(list(set([f.split('_')[0] for f in os.listdir(data_dir) if mode in f])))\n",
    "        for label_name in unique_label_names:\n",
    "            if label_name not in self.label_to_id:\n",
    "                self.label_to_id[label_name] = current_id\n",
    "                current_id += 1\n",
    "        \n",
    "        print(f\"Discovered {len(self.label_to_id)} unique labels for {mode} mode.\")\n",
    "\n",
    "        for file_name in sorted(os.listdir(data_dir)):\n",
    "            if mode in file_name:\n",
    "                label_name = file_name.split('_')[0]\n",
    "                label_id = self.label_to_id[label_name]\n",
    "                \n",
    "                file_path = os.path.join(data_dir, file_name)\n",
    "                \n",
    "                #\n",
    "                temp_tensor_shape = torch.load(file_path).shape\n",
    "                num_chunks_in_file = temp_tensor_shape[0]\n",
    "\n",
    "                for chunk_idx in range(num_chunks_in_file):\n",
    "                    self.data_info.append((file_path, chunk_idx, label_id))\n",
    "        \n",
    "        print(f\"Loaded {len(self.data_info)} total {mode} chunks.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, chunk_idx, label_id = self.data_info[idx]\n",
    "        \n",
    "\n",
    "        full_class_tensor = torch.load(file_path)\n",
    "        \n",
    "        data_chunk = full_class_tensor[chunk_idx]\n",
    "\n",
    "        \n",
    "        data_chunk = data_chunk.to(torch.float32) \n",
    "        \n",
    "\n",
    "        \n",
    "        label_tensor = torch.tensor(label_id, dtype=torch.long)\n",
    "        \n",
    "        return data_chunk, label_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = AstDataset('train', './data/train_chunks')\n",
    "test_dataset = AstDataset('test', './data/train_chunks')\n",
    "\n",
    "labels_for_model_init = np.array([info[2] for info in train_dataset.data_info])\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True) \n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True) \n",
    "\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "num_classes = len(train_dataset.label_to_id)\n",
    "model = transformers.ViTForImageClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "class_counts = np.bincount(labels_for_model_init) \n",
    "class_weights = 1. / (class_counts + 1e-9)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "best_f1 = 0\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n",
    "scaler = GradScaler() \n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    epoch_train_loss, epoch_test_loss = 0,0\n",
    "\n",
    "    for data, label in tqdm(train_loader, desc='Train-Batches'):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(dtype=torch.float16, device_type='cuda'):\n",
    "            train_output = model(data).logits\n",
    "            loss_val = loss_fn(train_output, label)\n",
    "        \n",
    "        epoch_train_loss += loss_val.item()\n",
    "        scaler.scale(loss_val).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    avg_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    print(f\"Average Epoch Train Loss of {epoch+1}th Epoch: {avg_epoch_train_loss:.4f}\")\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, label in tqdm(test_loader, desc='Test-Batches'):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            with autocast(dtype=torch.float16, device_type='cuda'): \n",
    "                test_output = model(data).logits\n",
    "                test_loss = loss_fn(test_output, label)\n",
    "            epoch_test_loss += test_loss.item()\n",
    "            prediction = test_output.argmax(dim=1)\n",
    "            all_preds.append(prediction.cpu().numpy())\n",
    "            all_labels.append(label.cpu().numpy())\n",
    "\n",
    "    all_preds, all_labels = np.concatenate(all_preds), np.concatenate(all_labels)\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    avg_epoch_test_loss = epoch_test_loss/len(test_loader)\n",
    "    print(f'Average Epoch Test Loss of {epoch+1}th Epoch: {avg_epoch_test_loss:.4f}')\n",
    "    print(f'Accuracy of Epoch {epoch+1}: {acc}')\n",
    "    print(f'F1-Score of Epoch {epoch+1}: {f1}')\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), './bestViT.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
