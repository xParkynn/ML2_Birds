{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/train.csv')\n",
    "\n",
    "data.drop(labels=['author', 'license', 'url', 'collection', 'common_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ce422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_dict = os.listdir('./data/train_audio')\n",
    "\n",
    "normalized_labels = dict()\n",
    "label_idx = 0\n",
    "for label in data['primary_label']:\n",
    "    if label in normalized_labels:\n",
    "        continue\n",
    "    else:\n",
    "        normalized_labels[label] = label_idx\n",
    "        label_idx += 1\n",
    "\n",
    "assert len(normalized_labels.keys()) == len(folder_dict)\n",
    "\n",
    "data['primary_label'] = data['primary_label'].map(lambda x: normalized_labels[x])\n",
    "\n",
    "label_to_name = dict()\n",
    "\n",
    "for idx, label in enumerate(data['primary_label']):\n",
    "    try:\n",
    "        name = data[data['primary_label'] == label]['scientific_name'][idx]\n",
    "        if label in label_to_name:\n",
    "            continue\n",
    "        label_to_name[label] = name\n",
    "    except:\n",
    "        print(label)\n",
    "\n",
    "label_to_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aadb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.drop('scientific_name', axis=1, inplace=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('label_to_name.json', 'w') as file:\n",
    "    json.dump(label_to_name, file)\n",
    "\n",
    "data.to_csv('processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "649bc952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/miniforge3/envs/nlp/lib/python3.12/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "CHUNK_SIZE = 5\n",
    "STRIDE = 2.5\n",
    "chunk_amount = int(32000*CHUNK_SIZE)\n",
    "stride_amount = int(32000*STRIDE)\n",
    "melspectogram = torchaudio.transforms.MelSpectrogram()\n",
    "amptodb = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "os.makedirs('./data/processed_train_audio', exist_ok=True)\n",
    "os.makedirs('./data/train_chunks', exist_ok=True)\n",
    "\n",
    "for label in os.listdir('./data/train_audio'):\n",
    "    train_chunks, test_chunks = [], []\n",
    "    files = os.listdir(f'./data/train_audio/{label}')\n",
    "    random.shuffle(files)\n",
    "    splitindex = int(len(files)*0.8)\n",
    "    train_files, test_files = files[:splitindex], files[splitindex:]\n",
    "\n",
    "    for file in train_files:\n",
    "        audio_file, sr = torchaudio.load(f'./data/train_audio/{label}/{file}')\n",
    "              \n",
    "        total_len = audio_file.shape[1]\n",
    "        for i in range(0, total_len - chunk_amount + 1, stride_amount):\n",
    "            chunk = audio_file[:, i:i+chunk_amount]\n",
    "            mel = melspectogram(chunk)\n",
    "            processed_chunk = amptodb(mel)\n",
    "            processed_chunk = (processed_chunk-processed_chunk.mean())/(processed_chunk.std()+1e-9)\n",
    "            train_chunks.append(processed_chunk)\n",
    "\n",
    "    for file in test_files:\n",
    "        audio_file, sr = torchaudio.load(f'./data/train_audio/{label}/{file}')\n",
    "              \n",
    "        total_len = audio_file.shape[1]\n",
    "        for i in range(0, total_len - chunk_amount + 1, stride_amount):\n",
    "            chunk = audio_file[:, i:i+chunk_amount]\n",
    "            mel = melspectogram(chunk)\n",
    "            processed_chunk = amptodb(mel)\n",
    "            processed_chunk = (processed_chunk-processed_chunk.mean())/(processed_chunk.std()+1e-9)\n",
    "            test_chunks.append(processed_chunk)\n",
    "\n",
    "    if len(train_chunks) < 100 or len(test_chunks) < 10:\n",
    "        continue\n",
    "        \n",
    "    if train_chunks and test_chunks:\n",
    "        random.shuffle(train_chunks)\n",
    "        random.shuffle(test_chunks)\n",
    "        if len(train_chunks) > 300:\n",
    "            indices = torch.randperm(len(train_chunks))[:300]\n",
    "            train_chunks = [train_chunks[i.item()] for i in indices]\n",
    "        if len(test_chunks) > 300:\n",
    "            indices = torch.randperm(len(test_chunks))[:300]\n",
    "            test_chunks = [test_chunks[i.item()] for i in indices]\n",
    "        train_tensor = torch.stack(train_chunks)\n",
    "        test_tensor = torch.stack(test_chunks)\n",
    "        train_tensor = F.interpolate(train_tensor, (224,224), mode='bilinear', align_corners=False)\n",
    "        test_tensor = F.interpolate(test_tensor, (224,224), mode='bilinear', align_corners=False)\n",
    "        train_tensor = train_tensor.repeat(1,3,1,1)\n",
    "        test_tensor = test_tensor.repeat(1,3,1,1)\n",
    "        torch.save(train_tensor.to(torch.float16), f'./data/train_chunks/{label}_train.pt')\n",
    "        torch.save(test_tensor.to(torch.float16), f'./data/train_chunks/{label}_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591dfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "\n",
    "def apply_augmentation(chunk):\n",
    "    if random.random() < 0.5:\n",
    "        chunk = T.FrequencyMasking(15)(chunk)\n",
    "    if random.random() < 0.5:\n",
    "        chunk = T.TimeMasking(35)(chunk)\n",
    "    return chunk\n",
    "\n",
    "for file in os.listdir('./data/train_chunks'):\n",
    "    if '_test' in file:\n",
    "        continue\n",
    "    augmented_chunks = []\n",
    "    tensor = torch.load(f'./data/train_chunks/{file}')\n",
    "    chunk_amount = tensor.shape[0]\n",
    "    augmented_chunks = []\n",
    "    idx = 0\n",
    "    \n",
    "    iter_counter = defaultdict(int)\n",
    "\n",
    "    while len(augmented_chunks) < 300-chunk_amount:\n",
    "        if iter_counter[idx] < 10:\n",
    "            augmented_chunks.append(apply_augmentation(tensor[idx]))\n",
    "            iter_counter[idx] += 1\n",
    "        idx = (idx + 1) % chunk_amount\n",
    "    if augmented_chunks:\n",
    "        final_tensor = torch.concat((tensor, torch.stack(augmented_chunks)))\n",
    "\n",
    "    torch.save(final_tensor, f'./data/train_chunks/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbac5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "\n",
    "CHUNK_SIZE = 5\n",
    "STRIDE = 2.5\n",
    "chunk_amount = int(32000*CHUNK_SIZE)\n",
    "stride_amount = int(32000*STRIDE)\n",
    "\n",
    "\n",
    "data = dict()\n",
    "\n",
    "#for label in os.listdir('./data/train_audio'):\n",
    "#    for file in os.listdir(f'./data/train_audio/{label}'):\n",
    "#        audio_file, sr = torchaudio.load(f'./data/train_audio/{label}/{file}')        \n",
    "#        total_len = audio_file.shape[1]\n",
    "#        number_of_chunks = 0\n",
    "#        for i in range(0, total_len - chunk_amount + 1, stride_amount):\n",
    "#            number_of_chunks += 1\n",
    "#        data[file[:-4]] = number_of_chunks\n",
    "\n",
    "for file in os.listdir('./data/train_chunks'):\n",
    "    tensor = torch.load(f'./data/train_chunks/{file}')\n",
    "    data[file[:-3]] = tensor.shape[0]\n",
    "\n",
    "with open('./dataset_init.json', 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910dbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_train, first_test = True, True\n",
    "for idx, file in enumerate(sorted(os.listdir('./data/processed_train_audio'))):\n",
    "    if '_train' in file:\n",
    "        train_tensor = torch.load(f'./data/processed_train_audio/{file}')\n",
    "        if first_train:\n",
    "            train_stacked_tensor = train_tensor\n",
    "            first_train=False\n",
    "        else:\n",
    "            train_stacked_tensor = torch.cat((train_stacked_tensor, train_tensor))\n",
    "    else:\n",
    "        test_tensor = torch.load(f'./data/processed_train_audio/{file}')\n",
    "        if first_test:\n",
    "            test_stacked_tensor = test_tensor\n",
    "            first_test = False\n",
    "        else:\n",
    "            test_stacked_tensor = torch.cat((test_stacked_tensor, test_tensor))\n",
    "\n",
    "torch.save(train_stacked_tensor, './data/processed_train_audio/train.pt')\n",
    "torch.save(test_stacked_tensor, './data/processed_train_audio/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f4478b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128, 313])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.load('./data/processed_train_audio/21116.pt').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257bfa1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/processed_train_audio/21116.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the tensor file (should have shape like [N, 1, 128, 801])\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/processed_train_audio/21116.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Pick a chunk to visualize (e.g., the first one)\u001b[39;00m\n\u001b[32m      8\u001b[39m chunk = tensor[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# shape: [1, 128, 801]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './data/processed_train_audio/21116.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the tensor file (should have shape like [N, 1, 128, 801])\n",
    "tensor = torch.load(\"./data/processed_train_audio/21038/iNat65519.pt\")\n",
    "\n",
    "# Pick a chunk to visualize (e.g., the first one)\n",
    "chunk = tensor[0]  # shape: [1, 128, 801]\n",
    "\n",
    "# Remove the channel dimension for plotting\n",
    "spectrogram = chunk.squeeze(0)  # shape: [128, 801]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(spectrogram, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(label='dB')\n",
    "plt.title(\"Mel Spectrogram\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel frequency bins\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a6662",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb65c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206/206 [18:02<00:00,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "CHUNK_LENGTH=5\n",
    "STRIDE=2.5\n",
    "\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=32000, n_fft=1024)\n",
    "amp_transform = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "\n",
    "def process_chunk(label_path, label):\n",
    "    chunks = []\n",
    "    for file in os.listdir(label_path):\n",
    "        path = f\"{label_path}/{file}\"\n",
    "        audio_file, sr = torchaudio.load(path)\n",
    "        total_len = audio_file.shape[1]\n",
    "        chunk_amount = int(sr * CHUNK_LENGTH)\n",
    "        stride_amount = int(sr*STRIDE)\n",
    "        if total_len < chunk_amount:\n",
    "            continue\n",
    "        for i in range(0, total_len - chunk_amount + 1, stride_amount):\n",
    "            chunk = audio_file[:, i:i+chunk_amount]\n",
    "\n",
    "            mel = mel_transform(chunk)\n",
    "            processed_chunk = amp_transform(mel)\n",
    "            chunks.append(processed_chunk)\n",
    "    if chunks:\n",
    "        torch.save(torch.stack(chunks), f\"./data/processed_train_audio/{label}.pt\")\n",
    "        del chunks, mel, processed_chunk, audio_file\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "for label in tqdm.tqdm(os.listdir('./data/train_soundscapes')):\n",
    "    process_chunk(f'./data/train_audio/{label}', label)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
