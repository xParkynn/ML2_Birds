{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe26fe16",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn               \n",
    "import torch.nn.functional as F       \n",
    "import torch.optim as optim            \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "#\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T   \n",
    "import numpy as np                     \n",
    "import pandas as pd                   \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from collections import OrderedDict, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09963657",
   "metadata": {},
   "source": [
    "# BASE MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98965b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # Dynamically determine the correct input size for the Linear layer\n",
    "        self.flattened_size = self._get_flattened_size()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def _get_flattened_size(self):\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, 128, 801)  # Same shape as input chunks\n",
    "            out = self.conv(dummy)\n",
    "            return out.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf8029",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85330c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "INIT_LR = 0.005\n",
    "BATCH_SIZE = 64\n",
    "LOSS = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = CNNClassifier(num_classes=206)\n",
    "optimizer = optim.SGD(model.parameters(), lr=INIT_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d50988",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c659348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectogramDataset(Dataset):\n",
    "    def __init__(self, audio_dir:str, label_to_idx:dict, max_cache_size: int = 5):\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.audio_dir = audio_dir\n",
    "        self.chunk_index_pairs = []\n",
    "        self.cache = OrderedDict()\n",
    "        self.path_to_label = defaultdict(list)\n",
    "        self.max_cache_size = max_cache_size\n",
    "\n",
    "        for label in os.listdir(self.audio_dir):\n",
    "            for file in os.listdir(f'{self.audio_dir}/{label}'):\n",
    "                tensor = torch.load(f\"{self.audio_dir}/{label}/{file}\")\n",
    "                amount_of_chunks = tensor.shape[0]\n",
    "                self.path_to_label[label].append(f'{self.audio_dir}/{label}/{file}')\n",
    "                for n in range(amount_of_chunks):\n",
    "                    self.chunk_index_pairs.append((f'./data/processed_train_audio/{label}/{file}' ,label, n))\n",
    "\n",
    "    def load_cached_tensor(self, file_path):\n",
    "        if file_path in self.cache:\n",
    "            self.cache.move_to_end(file_path)\n",
    "        else:\n",
    "            tensor = torch.load(file_path)\n",
    "            self.cache[file_path] = tensor\n",
    "            if len(self.cache) > self.max_cache_size:\n",
    "                self.cache.popitem(last=False)\n",
    "            return self.cache[file_path]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunk_index_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        file_path, label, chunk_index = self.chunk_index_pairs[idx]\n",
    "        tensor = self.load_cached_tensor(file_path)\n",
    "        chunk = tensor[chunk_index]\n",
    "\n",
    "        return chunk, self.label_to_idx[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20c400de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '100', '101', '102', '103', '104', '105', '106']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_csv(\"./data/processed_data.csv\")\n",
    "unique_labels = sorted(metadata[\"primary_label\"].astype(str).unique())\n",
    "label_to_index = {label: idx for idx,label in enumerate(unique_labels)}\n",
    "index_to_label = {idx: label for idx,label in enumerate(unique_labels)}\n",
    "print(list(unique_labels)[:10])\n",
    "#kann auch numerisch sorten theoretisch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71993924",
   "metadata": {},
   "source": [
    "## Create Data-Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10e222fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 21 s, total: 23.1 s\n",
      "Wall time: 1min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361766"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "audio_dir=\"./data/processed_train_audio\"\n",
    "\n",
    "\n",
    "train_dataset = SpectogramDataset(\n",
    "    audio_dir=audio_dir,\n",
    "    label_to_idx=label_to_index,\n",
    ")\n",
    "\n",
    "\n",
    "training_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609fe98",
   "metadata": {},
   "source": [
    "# Backup Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a97cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ChunkedSpectrogramDataset(Dataset):\n",
    "    def __init__(self, tensor_dir: str, label_to_idx: dict):\n",
    "        self.tensor_dir = tensor_dir\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.index_list = []\n",
    "\n",
    "        # üß† Indexstruktur aufbauen: [(label_name, chunk_idx), ...]\n",
    "        for fname in os.listdir(tensor_dir):\n",
    "            if fname.endswith(\".pt\"):\n",
    "                label = fname.replace(\".pt\", \"\")\n",
    "                tensor_path = os.path.join(tensor_dir, fname)\n",
    "                try:\n",
    "                    tensor = torch.load(tensor_path, map_location=\"cpu\")\n",
    "                    n_chunks = tensor.shape[0]\n",
    "                    for i in range(n_chunks):\n",
    "                        self.index_list.append((label, i))\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler beim Laden von {tensor_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_name, chunk_idx = self.index_list[idx]\n",
    "        tensor_path = os.path.join(self.tensor_dir, f\"{label_name}.pt\")\n",
    "\n",
    "        try:\n",
    "            # üîÅ Nur dieses eine Label laden\n",
    "            all_chunks = torch.load(tensor_path, map_location=\"cpu\")\n",
    "            chunk = all_chunks[chunk_idx]\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden von Chunk {chunk_idx} f√ºr Label {label_name}: {e}\")\n",
    "            chunk = torch.zeros((1, 128, 216))  # Dummy shape anpassen falls n√∂tig\n",
    "            \n",
    "        # üéØ Label-Tensor (One-hot)\n",
    "        label_tensor = torch.zeros(len(self.label_to_idx), dtype=torch.float32)\n",
    "        if label_name in self.label_to_idx:\n",
    "            label_index = self.label_to_idx[label_name]\n",
    "            label_tensor[label_index] = 1.0\n",
    "\n",
    "        return chunk, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b58905",
   "metadata": {},
   "source": [
    "# √úberarbeitetes  ChunkDataset von ino (path issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "daa62c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class inoChunkedSpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_list:list, label_to_idx: dict):\n",
    "        self.file_list = file_list\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.index_list = []\n",
    "\n",
    "        # üß† Indexstruktur aufbauen: [(label_name, chunk_idx), ...]\n",
    "        for tensor_path in self.file_list:\n",
    "            label = os.path.basename(tensor_path).replace(\".pt\", \"\")\n",
    "            try:\n",
    "                tensor = torch.load(tensor_path, map_location=\"cpu\")\n",
    "                n_chunks = tensor.shape[0]\n",
    "                for i in range(n_chunks):\n",
    "                    self.index_list.append((tensor_path, label, i))\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Laden von {tensor_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path, label_name, chunk_idx = self.index_list[idx]\n",
    "        try:\n",
    "            all_chunks = torch.load(tensor_path, map_location=\"cpu\")\n",
    "            chunk = all_chunks[chunk_idx]\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden von Chunk {chunk_idx} f√ºr Label {label_name}: {e}\")\n",
    "            chunk = torch.zeros((1, 128, 216))\n",
    "        label_tensor = torch.zeros(len(self.label_to_idx), dtype=torch.float32)\n",
    "        if label_name in self.label_to_idx:\n",
    "            label_index = self.label_to_idx[label_name]\n",
    "            label_tensor[label_index] = 1.0\n",
    "            \n",
    "        return chunk, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478682e6",
   "metadata": {},
   "source": [
    "# DataLoader for ChunkSpectogramDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f31545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crbtan1', '48124', '476537', '66016', '42087', 'crcwoo1', 'blcant4', '787625', '24292', '21116', '46010', 'compau', 'gybmar', '50186', 'brtpar1', 'whwswa1', '52884', '868458', 'royfly1', 'cinbec1', '963335', '476538', 'leagre', 'greibi1', 'ampkin1', 'plukit1', 'greani1', 'savhaw1', '22333', 'rosspo1', 'yelori1', 'recwoo1', 'rutjac1', '41970', 'baymac', 'butsal1', '555142', 'grnkin', '21038', '41778', 'cotfly1', 'yebfly1', 'bafibi1', 'amakin1', '548639', 'greegr', '66531', 'blbgra1', 'norscr1', 'spepar1', 'y00678', '24322', 'smbani', '1139490', '65349', 'watjac1', '65962', '21211', 'laufal1', '67252', '65336', 'strcuc1', '66578', 'spbwoo1', 'amekes', 'whttro1', 'trokin', 'yehbla2', 'blkvul', 'grekis', 'ywcpar', 'sahpar1', '134933', 'fotfly', 'strfly1', '42113', 'speowl1', 'gohman1', '566513', 'blcjay1', '715170', 'rtlhum', 'bucmot3', 'chbant1', '47067', 'stbwoo2', '135045', 'whtdov', 'sobtyr1', 'turvul', 'piwtyr1', 'cregua1', 'whbman1', '1462711', '22973', 'rugdov', 'yehcar1', 'cargra1', '523060', 'bobfly1', 'blctit1', 'trsowl', 'paltan1', 'bicwre1', 'rufmot1', '65448', '24272', 'piepuf1', 'yebsee1', 'pavpig2', '41663', 'anhing', 'bbwduc', 'chfmac1', 'cocwoo1', 'thlsch3', 'rinkin1', 'snoegr', 'bkcdon', 'purgal2', 'bobher1', 'bkmtou1', 'bugtan', 'rebbla1', 'orcpar', 'gretin1', '1462737', '1564122', 'linwoo1', 'pirfly1', '65373', 'olipic1', 'soulap1', '126247', 'thbeup1', 'rubsee1', '81930', 'socfly1', 'blhpar1', '67082', 'grasal4', 'bubcur1', '65547', 'roahaw', '65344', '1192948', 'tbsfin1', 'colara1', 'neocor', '1194042', '64862', 'labter1', '714022', 'palhor2', 'yercac1', 'plctan1', 'plbwoo1', 'compot1', 'colcha1', 'mastit1', 'strowl1', 'tropar', 'banana', 'bubwre1', 'shtfly1', 'grepot1', 'cocher1', 'yeofly1', 'whmtyr1', 'solsan', 'whfant1', 'blbwre1', '517119', 'grysee1', 'srwswa1', 'creoro1', 'ragmac1', 'woosto', 'grbhaw1', 'eardov1', 'littin1', 'secfly1', 'yebela1', 'shghum1', 'yecspi2', 'babwar', 'whbant1', '42007', '1346504', 'verfly', '65419', 'saffin', 'rumfly1', '555086', 'cattyr', '528041', 'strher', 'blchaw1', 'gycwor1', 'wbwwre1', 'crebob1', 'yectyr1', '22976', '66893', 'ruther1', 'rutpuf1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"./data/processed_train_audio/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b060cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_csv(\"./data/processed_data.csv\")\n",
    "unique_labels = sorted(metadata[\"primary_label\"].astype(str).unique())\n",
    "label_to_idx = {label: idx for idx,label in enumerate(unique_labels)}\n",
    "print(len(label_to_idx.keys()))\n",
    "\n",
    "tensor_dir = \"./data/processed_train_audio/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7cc54b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([151, 1, 128, 801])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the tensor from file\n",
    "tensor = torch.load('./data/processed_train_audio/21038/iNat65519.pt')\n",
    "# Print the shape\n",
    "print(tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe095532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "label_to_idx = label_to_idx\n",
    "tensor_dir = \"./data/processed_train_audio/\"\n",
    "#all file paths\n",
    "all_files = []\n",
    "for root, _, files in os.walk(tensor_dir):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\".pt\"):\n",
    "            all_files.append(os.path.join(root, fname))\n",
    "\n",
    "random.shuffle(all_files)\n",
    "split_idx = int(0.8 * len(all_files))\n",
    "train_files = all_files[split_idx:]\n",
    "test_files = all_files[:split_idx]\n",
    "\n",
    "\n",
    "train_dataset = inoChunkedSpectrogramDataset(train_files, label_to_idx)\n",
    "test_dataset = inoChunkedSpectrogramDataset(test_files, label_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6c344",
   "metadata": {},
   "source": [
    "# DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97d5a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2629b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931e2b4",
   "metadata": {},
   "source": [
    "# Train Loop inoChunkedSpectogramDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2965946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, Test loss: 0.0000, Accuracy: 290186/290186 (100%)\n",
      "\n",
      "\n",
      "Epoch: 1, Test loss: 0.0000, Accuracy: 290186/290186 (100%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_loss\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:211\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:212\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ML_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss_fn = LOSS\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "\n",
    "            # Multi-label prediction: sigmoid + threshold\n",
    "            pred = (torch.sigmoid(output) > 0.5).float()\n",
    "\n",
    "            # Count sample-wise full matches (optional: can do partial matching if needed)\n",
    "            correct += (pred == target).all(dim=1).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    print(\"\\nEpoch: {}, Test loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "        epoch, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35904da2",
   "metadata": {},
   "source": [
    "# Map von id to name f√ºr die Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e939abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1139490', 'Ragoniella pulchella'), ('1192948', 'Oxyprora surinamensis'), ('1194042', 'Copiphora colombiae'), ('126247', 'Spotted Foam-nest Frog'), ('1346504', 'Neoconocephalus brachypterus')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "taxonomy_df = pd.read_csv(\"./data/taxonomy.csv\")\n",
    "\n",
    "id_to_name = {}\n",
    "for index, row in taxonomy_df.iterrows():\n",
    "    primary_label = row[\"primary_label\"]\n",
    "    common_name = row[\"common_name\"] \n",
    "    id_to_name[primary_label] = common_name\n",
    "\n",
    "#f√ºr die visualisierung sp√§ter \n",
    "\n",
    "print(list(id_to_name.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531db48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
